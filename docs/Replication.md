
# Replication
<standard name="Replication">


*<desc>Study that deliberately repeats a previous study (i.e., original study) for the purpose of determining whether the results of the first study can be reproduced</desc>*


## Application 

This standard applies to empirical studies that meet the following criteria:

- The replication attempt is deliberate and planned, not an artifact of accidental overlap with a previous study.
- The original study is clearly identified as a separate previous publication. If the replication is not the only replication of the original study, i.e., it is a part of a *family of replications*, all the other replications are identified and the current replication is clearly defined in the context of the family.

## Specific Attributes
### Essential
<checklist name="Essential">

<intro>

- [ ] discusses the motivation for conducting the replication in appropriate detail (e.g. to validate the results, to broaden the results by changing the participant pool or the artifacts)
- [ ] defines the type of the replication by methodological similarity (exact, methodological, conceptual)<sup>[1](#footnote1)</sup>, participants (internal, external, mixed)<sup>[2](#footnote2)</sup>, and overlap (partial, complete, extended)<sup>[3](#footnote3)</sup>.

<method>

- describes the original study in appropriate detail, including
  - [ ] the research questions of the original study
  - [ ] the design of the original study 
  - [ ] if applicable, the participants of the original study (their number and any relevant characteristics)
  - [ ] the artifacts of the original study
  - [ ] the context variables of the original study that might have affected the design of the study or interpretation of the results
  - [ ] the major findings of the original study
  - [ ] if the replication is not the only replication of the original study, i.e., it is a part of a *family of replications*, a brief summary of the previous studies and replications is provided, including information about how the studies were related, conclusions drawn and current state of knowledge about the topic
- [ ] describes the level of interaction with the original experimenters if the original researchers are not involved in the replication
- [ ] describes and motivates the changes to the original experiment (design, participants, artifacts, procedures, data collected, and/or analysis techniques)

<results>

- [ ] compares the results of the replication to the results of the original study
- [ ] differentiates between results that are (i) consistent and (ii) inconsistent with the original study
- [ ] if the replication is not the only replication of the original study, i.e., it is a part of a *family of replications*, the results are placed into the context of the entire family of studies and conclusions are drawn based on knowledge gained by analyzing the results of all studies

<discussion>

<conclusion>

</checklist>

### Desirable
<checklist name="Desirable">

- [ ] reporting of information about (i) the original study, (ii) the replication, (iii) the comparison of results, and (iv) conclusions are clearly separated<sup>[4](#footnote4)</sup>

<method>

<results>

<discussion>

- [ ] draws conclusions across the two studies, providing insights that would not have been evident from either study individually
- [ ] highlights any conclusions of the original study that were strengthened

</checklist> 


### Extraordinary
<checklist name="Extraordinary">

<results>

- [ ] differentiates between minor and major differences when discussing inconsistent results, elaborates on whether the differences are reasonable and/or expected

<discussion>

- [ ] proposes hypotheses about new context variables that may have become evident through the analysis of multiple studies

</checklist> 

## Invalid Criticisms
- Lack of results that are inconsistent with the original study; the replication merely confirms the findings of the original study.
- Lack of public data or replication package when the replication package of the original study is not publicly available either.
- Criticizing the research questions, their wording, or formulation. Since the research question have been set in the original study, the replication must follow them, even in conceptual replications.

## Suggested Readings
- S. Heckman, J. C. Carver, M. Sherriff, and A. Al-zubidy, “A Systematic Literature Review of Empiricism and Norms of Reporting in Computing Education Research Literature,” ACM Transactions on Computing Education, vol. 22, no. 1. Association for Computing Machinery (ACM), pp. 1–46, Mar. 31, 2022. doi: 10.1145/3470652.
- F. Q. B. da Silva et al., “Replication of empirical studies in software engineering research: a systematic mapping study,” Empirical Software Engineering. Springer Science and Business Media LLC, Sep. 01, 2012. doi: 10.1007/s10664-012-9227-7.
- B. Kitchenham, “The role of replications in empirical software engineering—a word of warning,” Empirical Software Engineering, vol. 13, no. 2. Springer Science and Business Media LLC, pp. 219–221, Jan. 29, 2008. doi: 10.1007/s10664-008-9061-0.

## Exemplars
- D. Fucci and B. Turhan, “On the role of tests in test-driven development: a differentiated and partial replication,” Empirical Software Engineering, vol. 19, no. 2. Springer Science and Business Media LLC, pp. 277–302, Jun. 21, 2013. doi: 10.1007/s10664-013-9259-7.
- C. Apa, O. Dieste, E. G. Espinosa G., and E. R. Fonseca C., “Effectiveness for detecting faults within and outside the scope of testing techniques: an independent replication,” Empirical Software Engineering, vol. 19, no. 2. Springer Science and Business Media LLC, pp. 378–417, Aug. 08, 2013. doi: 10.1007/s10664-013-9267-7.
- J. Itkonen and M. V. Mäntylä, “Are test cases needed? Replicated comparison between exploratory and test-case-based software testing,” Empirical Software Engineering, vol. 19, no. 2. Springer Science and Business Media LLC, pp. 303–342, Jul. 11, 2013. doi: 10.1007/s10664-013-9266-8.
- M. N. Gómez and S. T. Acuña, “A replicated quasi-experimental study on the influence of personality and team climate in software development,” Empirical Software Engineering, vol. 19, no. 2. Springer Science and Business Media LLC, pp. 343–377, Aug. 02, 2013. doi: 10.1007/s10664-013-9265-9.
- F. Khan, I. David, D. Varro, and S. McIntosh, “Code Cloning in Smart Contracts on the Ethereum Platform: An Extended Replication Study,” IEEE Transactions on Software Engineering. Institute of Electrical and Electronics Engineers (IEEE), pp. 1–13, 2022. doi: 10.1109/tse.2022.3207428.

---
<footnote><sup>[1](#footnote1)</sup>A. R. Dennis and J. S. Valacich, “A replication manifesto,” AIS Transactions on Replication Research, vol. 1, no. 1, p. 1, 2015</footnote><br/>
<footnote><sup>[2](#footnote2)</sup>F. Q. B. da Silva et al., “Replication of empirical studies in software engineering research: a systematic mapping study,” Empirical Software Engineering. Springer Science and Business Media LLC, Sep. 01, 2012. doi: 10.1007/s10664-012-9227-7.</footnote><br/>
<footnote><sup>[3](#footnote3)</sup>J. C. Carver, N. Juristo, M. T. Baldassarre, and S. Vegas, “Replications of software engineering experiments,” Empirical Software Engineering, vol. 19, no. 2. Springer Science and Business Media LLC, pp. 267–276, Dec. 05, 2013. doi: 10.1007/s10664-013-9290-8.</footnote><br/>
<footnote><sup>[4](#footnote4)</sup>J. C. Carver, “Towards reporting guidelines for experimental replications: A proposal,” in 1st International Workshop on Replication in Empirical Software Engineering Research, vol. 1, 2010, pp. 1–4.
</footnote><br>
