# Qualitative Methods

## Action Research

#### *Empirical research that investigates how an intervention, like the introduction of a method or tool, affects a* real-life context

> Application
>
> This standard applies to empirical research that meets the following
> conditions.

-   investigates a primarily social phenomenon within its real-life,
    > organizational context

-   intervenes in the real-life context (otherwise see the **Case Study
    > Standard**)

-   the change and its observation are an integral part of addressing
    > the research question and contribute to research

> If the intervention primarily alters social phenomena (e.g. the
> organization's processes, culture, way of working or group dynamics),
> use this standard. If the intervention is a new technology or
> technique (e.g. a testing tool, a coding standard, a modeling
> grammar), especially if it lacks a social dimension, consider the
> **Engineering Research Standard**. If the research involves creating a
> technology and an organizational intervention with a social dimension,
> consider both standards.
>
> S[pecific Attributes]{.ul}

##### Importance Attribute

> Essential  describes the context or site of the intervention(s)

-   describes the intervention(s) in detail

-   describes the relationship between the researcher and the host
    > organization^5^

-   reports the length of the project and describes the longitudinal
    > dimension of the research design

-   describes the interactions between researcher(s) and host
    > organization(s)---what the interventions were, who intervened and
    > with which part of the organization, as well as the outcome of the
    > interventions

-   describes how interventions were determined (by management,
    > researchers, or participative/co- determination process)

-   explains research cycles or phases, if any, and their relationship
    > to the intervention(s)^6^

-   explains how the interventions are evaluated^7^

-   reports participant or stakeholder reactions to interventions

-   presents a clear and well-argued chain-of-evidence from observations
    > to findings

-   reports lessons learned by the organization

-   researchers reflect on their own possible biases

> Desirable  uses direct quotations extensively

-   uses member checking to assess resonance

-   findings plausibly transferable to other contexts

-   triangulation across quantitative and qualitative data

> Extraordinary  research team with triangulation across researchers
> (to mitigate researcher bias)

### General Quality Criteria

> Example criteria include reflexivity, credibility, resonance,
> usefulness and transferability (see **Glossary**). Positivist quality
> criteria such as internal validity, construct validity,
> generalizability and reliability typically do not apply.

### Examples of Acceptable Deviations

-   In a study of deviations from organizational standards, detailed
    > description of circumstances and direct quotations are omitted to
    > protect participants.

-   The article reports a negative outcome of an intervention and e.g.
    > investigates why a certain method was not applicable.

### Antipatterns

-   Forcing interventions that are not acceptable to participants or the
    > host organization.

-   Losing professional distance; becoming unable to evaluate the
    > intervention impartially; going native.

-   Over-selling a tool or method without regard for participants'
    > problems, practices or values.

> 5 E.g. project financing, potential conflicts of interest,
> professional relationship leading to access
>
> 6 Action research projects are structured in interventions often
> described as action research cycles, which are often structured in
> distinct phases. It is a flexible methodology, where subsequent cycles
> are based on their predecessors.
>
> 7 Can include quantitative evaluation in addition to qualitative
> evaluation.

-   Avoiding systematic evaluation; downplaying problems; simply
    > reporting participants views of the intervention.

### Invalid Criticisms

-   The findings and insights are not valid because the research
    > intervened in the context. Though reflexivity is crucial, the
    > whole point of action research is to introduce a change and
    > observe how participants react.

-   This is merely consultancy or an experience report. Systematic
    > observation and reflection should not be dismissed as consultancy
    > or experience reports. Inversely, consultancy or experiences
    > should not be falsely presented as action research.

-   Lack of quantitative data; causal analysis; objectivity, internal
    > validity, reliability, or generalizability.

-   Sample not representative; lack of generalizability; generalizing
    > from one organization.

-   Lack of replicability or reproducibility; not releasing transcripts.

-   Lack of control group or experimental protocols. An action research
    > study is not an experiment.

### Suggested Readings

> Richard Baskerville and A. Trevor Wood-Harper. 1996. A critical
> perspective on action research as a method for information systems
> research.\" *Journal of information Technology* 11.3, 235-246.
>
> Peter Checkland and Sue Holwell. 1998. Action Research: Its Nature and
> Validity. *Systematic Practice and Action Research.* (Oct.
>
> 1997), 9--21.
>
> Yvonne Dittrich. 2002. Doing Empirical Research on Software
> Development: Finding a Path between Understanding, Intervention, and
> Method Development. In *Social thinking---Software practice*. 243--262
>
> Yvonne Dittrich, Kari Rönkkö, Jeanette Eriksson, Christina Hansson and
> Olle Lindeberg. 2008. Cooperative method development.
>
> *Empirical Software Engineering.* 13, 3, 231-260. DOI:
> 10.1007/s10664-007-9057-1
>
> Kurt Lewin. 1947. Frontiers in Group Dynamics. *Human Relations* 1, 2
> (1947), 143--153. DOI: 10.1177/001872674700100201
>
> Lars Mathiassen. 1998. Reflective systems development. *Scandinavian
> Journal of Information Systems* 10, 1 (1998), 67--118
>
> Lars Mathiassen. 2002. Collaborative practice research. *Information,
> Technology & People.* 15,4 (2002), 321--345
>
> Lars Mathiassen, Mike Chiasson, and Matt Germonprez. 2012. Style
> Composition in Action Research Publication. *MIS quarterly. JSTOR*
>
> 36, 2 (2012), 347-363
>
> Miroslaw Staron. Action research in software engineering: Metrics'
> research perspective. *International Conference on Current Trends in
> Theory and Practice of Informatics.* (2019), 39-49
>
> Maung K. Sein, Ola Henfridsson, Sandeep Purao, Matti Rossi and Rikard
> Lindgren. 2011. Action design research. *MIS quarterly*. (2011),
> 37-56. DOI: 10.2307/23043488

### Exemplars

> Yvonne Dittrich, Kari Rönkkö, Jeanette Eriksson, Christina Hansson and
> Olle Lindeberg. 2008. Cooperative method development.
>
> *Empirical Software Engineering*. 13, 3 (Dec. 2007), 231-260. DOI:
> 10.1007/s10664-007-9057-1
>
> Helle Damborg Frederiksen, Lars Mathiassen. 2005. Information-centric
> assessment of software metrics practices. IEEE Transactions on
> Engineering Eanagement. 52, 3 (2005), 350-362. DOI:
> 10.1109/TEM.2005.850737
>
> Jakob Iversen and Lars Mathiassen. 2003. Cultivation and engineering
> of a software metrics program. Information Systems Journal. 13, 1
> (2006), 3--19
>
> Jakob Iversen. 1998. Problem diagnosis software process improvement.
> Larsen TJ, Levine L, DeGross JI (eds) Information systems: current
> issues and future changes.
>
> Martin Kalenda, Petr Hyna, Bruno Rossi. *Scaling agile in large
> organizations: Practices, challenges, and success factors*. Journal of
> Software: Evolution and Process. Wiley Online Library 30, 10 (Oct.
> 2018), 1954 pages.
>
> Miroslaw Ochodek, Regina Hebig, Wilhem Meding, Gert Frost, Miroslaw
> Staron. Recognizing lines of code violating company-specific coding
> guidelines using machine learning. *Empirical Software Engineering*.
> 25, 1 (Jan. 2020), 220-65.
>
> Kari Rönkkö, Brita Kilander, Mats Hellman, Yvonne Dittrich. 2004.
> Personas is not applicable: local remedies interpreted in a wider
> context. In *Proceedings of the eighth conference on Participatory
> design: Artful integration: interweaving media, materials and
> practices-Volume 1, Toronto, ON*, 112--120.
>
> Thatiany Lima De Sousa, Elaine Venson, Rejane Maria da Costa
> Figueired, Ricardo Ajax Kosloski, and Luiz Carlos Miyadaira Ribeiro.
>
> Using Scrum in Outsourced Government projects: An Action Research.
> 2016. In *2016 49th Hawaii International Conference on System Sciences
> (HICSS)*, January 5, 2016, 5447-5456.
>
> Hataichanok Unphon, Yvonne Dittrich. 2008. Organisation matters: how
> the organisation of software development influences the introduction
> of a product line architecture. In *Proc. IASTED Int. Conf. on
> Software Engineering*. 2008, 178-183

## Case Study

#### *"An empirical inquiry that investigates a contemporary phenomenon (the "case") in depth and within its real-* world context, especially when the boundaries between phenomenon and context \[are unclear\]" (Yin 2017)

> Application
>
> This standard applies to empirical research that meets the following
> conditions.

-   Presents a detailed account of a specific instance of a *phenomenon*
    > at a *site*. The phenomenon can be virtually anything of interest
    > (e.g. Unix, cohesion metrics, communication issues). The site can
    > be a community, an organization, a team, a person, a process, an
    > internet platform, etc.

-   Features direct or indirect observation (e.g. interviews, focus
    > groups)---see Lethbridge et al.'s (2005) taxonomy.

-   Is not an experience report (cf. Perry et al. 2004) or a series of
    > shallow inquiries at many different sites.

> A case study can be brief (e.g. a week of observation) or longitudinal
> (if observation exceeds the natural rhythm of the site; e.g.,
> observing a product over many releases). For our purposes, *case
> study* subsumes ethnography.
>
> If data collection and analysis are interleaved, consider the
> **Grounded Theory Standard**. If the study mentions action research,
> or intervenes in the context, consider the **Action Research
> Standard.** If the study captures a large quantitative dataset with
> limited context, consider the **Exploratory Data Science Standard.**

S[pecific Attributes]{.ul}

##### Importance Attribute

> Essential  explains why the case study approach is appropriate for
> the research question

-   justifies the selection of the case or site that was studied

-   describes the context of the case in rich detail

-   defines unit(s) of analysis

-   presents a clear and well-argued "chain of evidence" from
    > observations to findings

-   clearly answers the research question(s)

> Desirable  reports the type of case study (see *Types of Case
> Studies*, below)

-   describes external events and other factors that may have affected
    > the case or site

-   explains how researchers triangulated across data sources,
    > informants or researchers

-   cross-checks interviewee statements (e.g. against direct observation
    > or archival records)

-   uses quotations to *illustrate* findings (note: quotations should
    > not be *the only* representation of a finding; each finding should
    > be described independently of supporting quotations)

> Extraordinary  multiple, deep, fully-developed cases with cross-case
> triangulation

-   uses multiple judges and reports inter-rater reliability (cf. Gwet &
    > Gwet 2002)

-   uses direct observation and clearly integrates direct observations
    > into results

-   created a case study protocol beforehand and makes it publicly
    > accessible

### General Quality Criteria

> Case studies should be evaluated using qualitative validity criteria
> such as credibility, multivocality, reflexivity, rigor and
> transferability (see **Glossary**). Quantitative quality criteria such
> as replicability, generalizability and objectivity typically do not
> apply.

### Types of Case Studies

> There is no standard way of conducting a case study. Case study
> research can adopt different philosophies, most notably
> (post-)positivism (Lee 1989) and interpretivism/constructivism
> (Walsham 1995), and serve different purposes, including:

-   a **descriptive case study** describes---in vivid detail--a
    > particular instance of a phenomenon

-   an **emancipatory case study** identifies social, cultural, or
    > political domination "that may hinder human ability" (Runeson and
    > Host 2009), commensurate with a critical epistemological stance

-   an **evaluative case study** evaluates a priori research questions,
    > propositions, hypotheses or technological artifacts

-   an **explanatory case study** explains how or why a phenomenon
    > occurred, typically using a process or variance theory

-   an **exploratory case study** explores a particular phenomenon to
    > identify new questions, propositions or hypotheses

-   an **historical case study** draws on archival data, for instance,
    > software repositories

-   a **revelatory case study** examines a hitherto unknown or
    > unexplored phenomenon

### Invalid Criticisms

-   Does not present quantitative data; only collects a single data
    > type.

-   Sample of 1; findings not generalizable. The point of a case study
    > is to study one thing deeply, not to generalize to a population.
    > Case studies should lead to theoretical generalization; that is,
    > concepts that are transferable in principle.

-   Lack of internal validity. Internal validity only applies to
    > explanatory case studies that seek to establish causality.

-   Lack of reproducibility or a "replication package"; Data are not
    > disclosed (qualitative data are often confidential).

-   Insufficient number of length of interviews. There is no magic
    > number; what matters is that there is enough data that the
    > findings are credible, and the description is deep and rich.

### Exemplars

> Adam Alami, and Andrzej Wąsowski. 2019. Affiliated participation in
> open source communities. In *2019 ACM/IEEE International Symposium on
> Empirical Software Engineering and Measurement (ESEM)*. 1-11
>
> Michael Felderer and Rudolf Ramler. 2016. Risk orientation in software
> testing processes of small and medium enterprises: an exploratory and
> comparative study. *Software Quality Journal*. 24, 3 (2016), 519-548.
>
> Audris Mockus, Roy T. Fielding, and James D. Herbsleb. 2002. Two case
> studies of open source software development: Apache and Mozilla. *ACM
> Transactions on Software Engineering and Methodology (TOSEM).* 11, 3
> (2002), 309-346.
>
> Helen Sharp and Hugh Robinson. 2004. An ethnographic study of XP
> practice. *Empirical Software Engineering.* 9, 4 (2004), 353-375.
>
> Diomidis Spinellis and Paris C. Avgeriou. Evolution of the Unix System
> Architecture: An Exploratory Case Study. *IEEE Transactions on
> Software Engineering*. (2019).
>
> Klaas-Jan Stol and Brian Fitzgerald. Two's company, three's a crowd: a
> case study of crowdsourcing software development. In
>
> *Proceedings of the 36^th^ International Conference on Software
> Engineering*, 187--198, 2014.

### Suggested Readings

> Line Dube and Guy Pare. Rigor in information systems positivist case
> re-search: current practices, trends, and recommendations. 2003.
>
> *MIS quarterly.* JSTOR 27, 4 (Dec. 2003), 597--636. DOI:
> 10.2307/30036550
>
> Shiva Ebneyamini, and Mohammad Reza Sadeghi Moghadam. 2018. Toward
> Developing a Framework for Conducting Case Study Research.
> *International Journal of Qualitative Methods.* 17, 1 (Dec. 2018)
>
> Kilem Gwet. 2002. Inter-Rater Reliability: Dependency on Trait
> Prevalence and Marginal Homogeneity. Statistical Methods for Inter-
> Rater Reliability Assessment Series, 2 (May 2002), 9 pages.
>
> Barbara Kitchenham, Lesley Pickard, and Shari Lawrence Pfleeger. 1995.
> Case studies for method and tool evaluation. *IEEE software.*
>
> 12, 4 (1995), 52-62.
>
> Timothy C. Lethbridge, Susan Elliott Sim, and Janice Singer. 2005.
> Studying software engineers: Data collection techniques for software
> field studies. *Empirical software engineering.* 10, 3 (2005),
> 311-341.
>
> Mathew Miles, A Michael Huberman and Saldana Johnny. 2014.
> *Qualitative data analysis: A methods sourcebook*.
>
> Dewayne E. Perry, Susan Elliott Sim, and Steve M. Easterbrook. 2004.
> Case Studies for Software Engineers, In *Proceedings 26th
> International Conference on Software Engineering.* 28 May 2008,
> Edinburgh, UK, 736-738.
>
> Per Runeson and Martin Höst. 2009. Guidelines for conducting and
> reporting case study research in software engineering. *Empirical
> software engineering*. 14, 2, 131 pages.
>
> Per Runeson, Martin Host, Austen Rainer, and Bjorn Regnell. 2012. Case
> study research in software engineering: Guidelines and examples. John
> Wiley & Sons.
>
> Sarah J. Tracy. 2010. Qualitative Quality: Eight "Big-Tent" Criteria
> for Excellent Qualitative Research. *Qualitative Inquiry*. *16*, 10,
> 837-- 851. DOI: [10.1177/1077800410383121]{.ul}
>
> Geoff Walsham, 1995. Interpretive case studies in IS research: nature
> and method. *European Journal of information systems.* 4,2, 74-81.
> Robert K. Yin. 2017. *Case study research and applications: Design and
> methods*. Sage publications.

## Grounded Theory

#### *A study of a specific area of interest or phenomenon that involves iterative and interleaved rounds of qualitative* data collection and analysis, leading to key patterns (e.g. concepts, categories)

> Application
>
> This standard applies to empirical inquiries that meet all of the
> following conditions:

-   Explores a broad area of investigation without specific, up-front
    > research questions.

-   Applies theoretical sampling with iterative and interleaved rounds
    > of data collection and analysis.

-   Reports rich and nuanced findings, typically including verbatim
    > quotes and samples of raw data.

> For predominately qualitative inquiries that do not iterate between
> data collection and analysis or do not use theoretical sampling,
> consider the **Case Study Standard** or the **Qualitative Survey
> Standard**.
>
> S[pecific Attributes]{.ul}

##### Importance Attribute

> Essential  identifies the version of Grounded Theory used/adapted
> (Glaser, Strauss-Corbin, Charmaz, etc.)

-   explains how data source(s) were selected and accessed (e.g.
    > participant sampling strategy)

-   explains how the research iterated between data collection and
    > analysis using constant comparison and theoretical sampling

-   provides evidence of saturation; explains how saturation was
    > achieved

-   explains how key patterns (e.g. categories) emerged from GT steps
    > (e.g. selective coding)

-   provides clear chain of evidence from raw data (e.g. interviewee
    > quotations) to derived codes, concepts, and categories

> Desirable  explains how and why study adapts or deviates from claimed
> GT version

-   presents a mature, fully-developed theory or taxonomy

-   includes highly diverse participants and/or data sources (e.g.
    > software repositories, forums)

-   uses direct quotations extensively to support key points

-   explains how memo writing was used to drive the work

-   validates results (e.g. member checking, feedback from
    > non-participant practitioners, research audits of coding with
    > advisors/other researchers)

-   includes supplemental materials such as interview guide(s), coding
    > schemes, coding examples, decision rules, or chain-of-evidence
    > tables too large for main text

-   discusses transferability; characterizes the setting such that
    > readers can assess transferability

-   compares results with (or integrates them into) prior theory or
    > related research

-   explains theoretical sampling vis-à-vis the interplay between the
    > sampling process, the emerging findings, and theoretical gaps
    > perceived therein

-   reflects on how researcher's biases may have affected their analysis

-   explains the role of literature, especially where an extensive
    > review preceded the GT study

> Extraordinary  triangulates with extensive quantitative data (e.g.
> questionnaires, sentiment analysis)

-   employs a team of researchers and explains their roles

### Quality Criteria

> Glaser, Strauss, Corbin and Charmaz advance inconsistent quality
> criteria. Using definitions in our **Glossary**, reviewers should
> consider common qualitative criteria such as **credibility**,
> **resonance**, **usefulness** and the degree to which results *extend*
> our cumulative knowledge. Quantitative quality criteria such as
> internal validity, construct validity, replicability, generalizability
> and reliability typically do not apply.

### Examples of Acceptable Deviations

-   In a study of sexual harassment at a named organization, detailed
    > description of interviewees and direct quotations are omitted to
    > protect participants.

### Antipatterns

-   Conducting data collection and data analysis sequentially; applying
    > only analysis techniques of GT.

-   Data analysis focusing on counting words, codes, concepts, or
    > categories instead of interpreting.

-   Presenting a tutorial on grounded theory instead of explaining how
    > the current study was conducted.

-   Small, heterogenous samples creating the illusion of convergence and
    > theoretical saturation. For example, it is highly unlikely that a
    > full theory can be derived only from interviews with 20 people.

-   Focusing only on interviews without corroborating statements with
    > other evidence (e.g. documents, observation).

### Invalid Criticisms

-   lack of quantitative data; causal analysis; objectivity, internal
    > validity, reliability, or generalizability

-   lack of replicability or reproducibility; not releasing transcripts

-   lack of representativeness (e.g. of a study of Turkish programmers,
    > 'how does this generalize to America?')

-   research questions should have been different

-   findings should have been presented as a different set of
    > relationships, hypotheses, or a different theory.

### Suggested Readings

> Steve Adolph, Wendy Hall, and Philippe Kruchten. 2011. Using grounded
> theory to study the experience of software development.
>
> *Empirical Software Engineering*. 16, 4 (2011), 487-513.
>
> Terry Rowlands, Neal Waddell, and Bernard McKenna. 2016. Are We There
> Yet? A Technique to Determine Theoretical Saturation.
>
> *Journal of Computer Information Systems*. 56, 1 (2016), 40-47.
>
> Klaas-Jan Stol, Paul Ralph, and Brian Fitzgerald. 2016. Grounded
> theory in software engineering research: a critical review and
> guidelines. In *Proceedings of the 38th International Conference on
> Software Engineering (ICSE '16)*. Association for Computing Machinery,
> New York, NY, USA, 120--131. DOI: 10.1145/2884781.2884833
>
> Juliet Corbin and Anselm Strauss. 2014. *Basics of qualitative
> research: Techniques and procedures for developing grounded theory*.
> Sge publications.
>
> Kathy Charmaz. 2014. *Constructing grounded theory*. sage.

### Exemplars

> Barthélémy Dagenais and Martin P. Robillard. 2010. Creating and
> evolving developer documentation: understanding the decisions of open
> source contributors. In Proceedings of the eighteenth ACM SIGSOFT
> international symposium on Foundations of software engineering (FSE
> '10). Association for Computing Machinery, New York, NY, USA,
> 127--136. DOI: 10.1145/1882291.1882312
>
> Rashina Hoda, James Noble, and Stuart Marshall. 2012. Self-organizing
> roles on agile software development teams. *IEEE Transactions on
> Software Engineering*. IEEE 39, 3 (May 2012), 422-444. DOI:
> 10.1109/TSE.2012.30
>
> Todd Sedano, Paul Ralph, and Cécile Péraire. 2017. Software
> development waste. In 2017 IEEE/ACM 39th International Conference on
> Software Engineering (ICSE). (May. 2017), 130-140. DOI: 10.1109/icse
> (2017).
>
> Christoph Treude and Margaret-Anne Storey. 2011. Effective
> communication of software development knowledge through community
> portals. In *Proceedings of the 19th ACM SIGSOFT symposium and the
> 13th European conference on Foundations of software engineering
> (ESEC/FSE '11)*. Association for Computing Machinery, New York, NY,
> USA, 91--101. DOI:10.1145/2025113.2025129
>
> Michael Waterman, James Noble, and George Allan. 2015. How much
> up-front? A grounded theory of agile architecture. In *2015 IEEE/ACM
> 37th IEEE International Conference on Software Engineering*. 1, (May
> 2015), 347-357.

## Qualitative Surveys (Interview Studies)

#### *A study comprising semi-structured or open-ended interviews*

> Application
>
> This standard applies to empirical inquiries that meet all of the
> following criteria:

-   Researcher(s) have synchronous conversations with one participant at
    > a time

-   Researchers ask, and participants answer, open-ended questions

-   Participants' answers are recorded in some way

-   Researchers apply some kind of qualitative data analysis to
    > participants' answers

> If researchers iterated between data collection and analysis, consider
> the **Grounded Theory Standard***.* If respondents are all from the
> same organization, consider the **Case Study Standard**. If
> researchers collect written text or conversations (e.g. StackExchange
> threads), consider the **Discourse Analysis Standard**.
>
> S[pecific Attributes]{.ul}

##### Importance Attribute

> Essential  explains how interviewees were selected (i.e. sampling
> strategy; see **The Sampling Supplement**)

-   describes interviewees (e.g. demographics, work roles)

-   presents clear chain of evidence from interviewee quotations to
    > proposed concepts

> Desirable  includes highly diverse participants

-   uses direct quotations extensively to support key points

-   EITHER: evaluates an a priori theory (or model, framework, taxonomy,
    > etc.) using deductive coding with an a priori coding scheme based
    > on the prior theory

> OR: synthesizes results into a new, mature, fully-developed and
> clearly articulated theory (or model, etc.) using some form of
> inductive coding (coding scheme generated from data)

-   validates results (e.g. using member checking)

-   EITHER uses audits (inductive coding)

> OR uses multiple coders and reports agreement statistics (deductive
> coding)

-   provides supplemental materials including interview guide(s), coding
    > schemes, coding examples, decision rules, extended
    > chain-of-evidence table(s)

-   discusses transferability; findings plausibly transferable to
    > different contexts

-   compares results with (or integrates them into) prior theory or
    > related research

-   reflects on how researchers' biases may have affected their analysis

> Extraordinary  employs multiple methods of data analysis (e.g. open
> coding vs. process coding; manual coding vs. automated sentiment
> analysis) with method-triangulation

-   employs longitudinal design (i.e. each interviewee participates
    > multiple times) and analysis

-   employs probabilistic sampling strategy; statistical analysis of
    > response bias

### General Quality Criteria

> An interview study should address appropriate qualitative quality
> criteria such as: **credibility, resonance**, **usefulness**, and
> **transferability** (see **Glossary**). Quantitative quality criteria
> such as internal validity, construct validity, generalizability and
> reliability typically do not apply.

### Examples of Acceptable Deviations

-   In a study of deaf software developers, the interviews are conducted
    > via text messages.

-   In a study of sexual harassment at named organizations, detailed
    > description of interviewees and direct quotations are omitted to
    > protect participants.

-   In a study of barriers faced by gay developers, participants are all
    > gay (but should be diverse on other dimensions).

### Antipatterns

-   Interviewing a small number of similar people, creating the illusion
    > of convergence and saturation

-   Mis-presenting a qualitative survey as grounded theory or a case
    > study.

### Invalid Criticisms

-   Lack of quantitative data; causal analysis; objectivity, internal
    > validity, reliability, or generalizability.

-   Lack of replicability or reproducibility; not releasing transcripts.

-   Lack of probability sampling, statistical generalizability or
    > representativeness unless representative sampling was an explicit
    > goal of the study.

-   Failure to apply grounded theory or case study practices. A
    > qualitative survey is not grounded theory or a case study.

### Notes

-   A qualitative survey generally has more interviews than a case study
    > that triangulates across different kinds of data.

### Suggested Readings

> Michael Quinn Patton. 2002. *Qualitative Research and Evaluation
> Methods*. 3rd ed. Sage Publications. Herbert J. Rubin and Irene S.
> Rubin. 2011. *Qualitative interviewing: The art of hearing data*.
> Sage.
>
> Johnny Saldaña. 2015. *The coding manual for qualitative researchers*.
> Sage.

### Exemplars

> Marian Petre. 2013. UML in practice. In *Proceedings of the 35th
> International Conference on Software Engineering,* San Francisco, USA,
> 722-731.
>
> Paul Ralph and Paul Kelly. 2014. The dimensions of software
> engineering success. In *Proceedings of the 36th International
> Conference on Software Engineering (ICSE 2014)*. Association for
> Computing Machinery, New York, NY, USA, 24--35. DOI:
> 10.1145/2568225.2568261
>
> Paul Ralph and Ewan Tempero. 2016. Characteristics of decision-making
> during coding. In *Proceedings of the 20th International Conference on
> Evaluation and Assessment in Software Engineering (EASE '16).*
> Association for Computing Machinery, New York, NY, USA, Article 34,
> 1--10. DOI:10.1145/2915970.2915990Evaluation and Assessment in Software
> Engineering (EASE).* (Jun. 2008), 1-10.