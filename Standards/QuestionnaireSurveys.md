# Questionnaire Surveys 
<standard name="Questionnaire Surveys">

A study in which a sample of respondents answer a series of questions,
typically through a computerized or paper form and mostly structured

## Application

This guideline applies to studies in which:

-   a sample of participants answer predefined, mostly closed-ended
    questions (typically online or on paper)

-   researchers systematically analyze participants' answers

Surveys can be descriptive, exploratory or confirmatory. Confirmatory
surveys can test individual propositions or complex theories. This
standard does not apply to questionnaires comprising predominately
open-ended questions[^23], literature surveys (see the **Systematic
Review Standard**), longitudinal or repeated measures studies (see the
**Longitudinal Studies Standard**), or the demographic questionnaires
typically given to participants in controlled experiments (see the
**Experiments Standard**).

## Specific Attributes 

### Essential Attributes 
<checklist name="Essential">

-	identifies the target population & defines the sampling strategy (see the Sampling Supplement)

-	provides questionnaire instrument (e.g. as supplemental file)

-	EITHER: provides study artifacts; i.e., instrument(s), code books, analysis scripts and dataset(s) (addressing potential anonymity and confidentiality issues) \
	 OR: describes in detail study artifacts and justifies why they are not provided

-	the questionnaire design matches the research aims (i.e. questions are mapped to research objectives) and the target population (wording and format of the questions)

-	describes how participants were selected, including invitations and incentives

-	step-by-step, systematic, replicable description of data collection and analysis 

-	describes how responses were managed/monitored, including contingency actions for non-responses and drop-outs

-	EITHER: measures constructs using (or adapting) validated scales  \
	 OR: analyzes construct validity (e.g. content, convergent, discriminant, predictive) ex post

-	explains handling of missing data (e.g. imputation, weighting adjustments, discarding)

-	acknowledges generalizability threats; discusses how respondents may differ from target population

-	analyzes response rates
</checklist>
     
### Desirable Attributes 	
<checklist name="Desirable">

-	characterizes the target population including demographic information (e.g. culture, knowledge)

-	defines and estimates the size of the population strata (if applicable)

-	accounts for the principles of research ethics (e.g. informed consent, re-identification risk)

-	explains and justifies instrument design and choice of scales (e.g. by research objectives or by analogy to similar studies).

-	validates whether the items, layout, duration, and technology are appropriate (e.g. using pilots, test-retest, or expert and non-expert reviews).

-	reports how the instrument has evolved through the validation process (if at all)

-	applies techniques for improving response rates (e.g. incentives, reminders, targeted advertising)

-	analyzes response bias (quantitatively)

-	discusses possible effect of incentives (e.g. on voluntariness, response rates, response bias) if used

-	describes the stratification of the analysis (if stratified sampling is used)

-	clearly distinguishes evidence-based results from interpretations and speculation
 </checklist>
     
### Extraordinary Attributes 	
<checklist name="Extraordinary">

-	provides feasibility check of the anticipated data analysis techniques

-	reports on the scale validation in terms of dimensionality, reliability, and validity of measures
</checklist>

## General Quality Criteria 

Survey studies should address quantitative quality criteria such
as **internal validity**, **construct validity**, **external validity**,
**reliability** and **objectivity** (see **Glossary**).

## Variations 

-   **Descriptive surveys** provide a detailed account of the properties
    of a phenomenon or population.

-   **Exploratory surveys** generate insights, hypotheses or models for
    further research.

-   **Confirmatory surveys** testing formal (e.g. causal) propositions
    to explain a phenomenon.

## Invalid Criticism 

-   Not reporting response rate for open public subscription surveys
    (i.e. surveys open to the anonymous public so that everyone with a
    link---typically broadcasted among social networks---can
    participate).

-   Failure to release full data sets despite the data being sensitive.

-   Claiming the sample size is too small without justifying why the
    sample size is insufficient to answer the research questions.

-   Criticizing the relevance of a survey on the basis that responses
    only capture general people's perceptions.

-   The results are considered controversial or hardly surprising.

-   The results do not accord with the reviewer's personal experience or
    previous studies.

## Suggested Readings 

Don Dillman, Jolene Smyth, and Leah Christian. 2014. Internet, phone,
mail, and mixed-mode surveys: the tailored design method. John Wiley &
Sons.

Mark Kasunic. 2005. Designing an effective survey. Tech report
\#CMU/SEI-2005-GB-004, Carnegie-Mellon University, Pittsburgh, USA.

Jefferson Seide Molléri, Kai Petersen, and Emilia Mendes. *An
empirically evaluated checklist for surveys in software engineering.*
Information and Software Technology*.* 119 (2020).

Stefan Wagner, Daniel Mendez, Michael Felderer, Daniel Graziotin, Marcos
Kalinowski. Challenges in Survey Research. In: Contemporary Empirical
Methods in Software Engineering, *Springer,* 2020.

Paul Ralph and Ewan Tempero. 2018. Construct Validity in Software
Engineering Research and Software Metrics. In *Proceedings of the 22nd
International Conference on Evaluation and Assessment in Software
Engineering* 2018 (EASE'18), 13–23. DOI:10.1145/3210459.3210461

Marco Torchiano, Daniel Méndez, Guilherme Horta Travassos, and Rafael
Maiani de Mello. 2017. Lessons learnt in conducting survey research. In
*Proceedings of the 5th International Workshop on Conducting Empirical
Studies in Industry (CESI '17)*, 33–39. DOI:10.1109/CESI.2017.5

Torchiano Marco and Filippo Ricca. Six reasons for rejecting an
industrial survey paper. In *2013 1st International Workshop on
Conducting Empirical Studies in Industry (CESI).* (2013), 21–26.

## Exemplars 

Jingyue Li, Reidar Conradi, Odd Petter Slyngstad, Marco Torchiano,
Maurizio Morisio, and Christian Bunse. A State-of-the-Practice Survey on
Risk Management in Development with Off-The-Shelf Software Components.
In *IEEE Transactions on Software Engineering*. 34, 2 (2008), 271–286.

D. Méndez Fernández, Stefan Wagner, Marcos Kalinowski, Michael Felderer,
Priscilla Mafra, Antonio Vetrò, Tayana Conte et al. Naming the Pain in
Requirements Engineering: Contemporary Problems, Causes, and Effects in
Practice. In *Empirical software engineering*. 22, 5 (2016), 2298–2338.

Paul Ralph, Sebastian Baltes, Gianisa Adisaputri, Richard Torkar,
Vladimir Kovalenko, Marcos Kalinowski, et al. Pandemic Programming: How
COVID-19 affects software developers and how their organizations can
help. In *Empirical Software Engineering*, 25, 6, 2020, 4927–4961. DOI:
10.1007/s10664-020-09875-y

Stefan Wagner, Daniel Méndez Fernández, Michael Felderer, Antonio Vetrò,
Marcos Kalinowski, Roel Wieringa, et al. 2019. Status Quo in
Requirements Engineering: A Theory and a Global Family of Surveys. *ACM
Trans. Softw. Eng. Methodol.* 28, 2, Article 9 (April 2019), 48 pages.
DOI:10.1145/3306607
</standard>